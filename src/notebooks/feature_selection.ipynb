{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-21T07:48:40.707425Z",
     "start_time": "2026-01-21T07:48:38.252882Z"
    }
   },
   "source": [
    "import warnings\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from src.util.constants import DATA_PATH\n",
    "from src.util.common import mean_grouped_spearman_correlation, save_as_pickle"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For feature selection, we use simple correlation scoring. For hyperparameter tuning, we'll switch to performance approximation.",
   "id": "6de2b86421b3389e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-21T07:48:54.633591Z",
     "start_time": "2026-01-21T07:48:44.651065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_train: pl.DataFrame = pl.read_parquet(f\"{DATA_PATH}/folds/df_train_0.parquet\")\n",
    "feature_names = [x for x in df_train.columns if 'feature' in x]\n",
    "del df_train\n",
    "number_of_shadow_features: int = round(len(feature_names) / 10)\n",
    "\n",
    "df_train_list = []\n",
    "df_validate_list = []\n",
    "\n",
    "def add_shadow_features(df: pl.DataFrame, _feature_names: list[str]) -> pl.DataFrame:\n",
    "    shadow_df = df.select([pl.col(col_name).shuffle().alias(f'{col_name}_shadow') for col_name in _feature_names])\n",
    "\n",
    "    return pl.concat([df, shadow_df], how=\"horizontal\")\n",
    "\n",
    "random_features = random.sample(feature_names, number_of_shadow_features)\n",
    "\n",
    "for fold in range(2):\n",
    "    df_train: pl.DataFrame = pl.read_parquet(f\"{DATA_PATH}/folds/df_train_{fold}.parquet\")\n",
    "    df_validate: pl.DataFrame = pl.read_parquet(f\"{DATA_PATH}/folds/df_validate_{fold}.parquet\")\n",
    "\n",
    "    df_train = add_shadow_features(df_train, random_features)\n",
    "    df_validate = add_shadow_features(df_validate, random_features)\n",
    "\n",
    "    df_train_list.append(df_train)\n",
    "    df_validate_list.append(df_validate)\n",
    "    del df_train, df_validate"
   ],
   "id": "4e09ed773c6f53d1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "feature_list_to_test = feature_names + [f'{feature}_shadow' for feature in random_features]\n",
    "selected_features = feature_names\n",
    "\n",
    "# run on a small set of hyperparameters\n",
    "num_boost_round_space = [50, 200]\n",
    "max_depth_space = [4, 6, 8]\n",
    "\n",
    "active = True\n",
    "best_mean_corr = -1.0\n",
    "best_max_corr = -1.0\n",
    "features_to_keep = feature_list_to_test\n",
    "features_to_keep_last = features_to_keep\n",
    "while active:\n",
    "    result_permutation_importance = []\n",
    "\n",
    "    corrs = []\n",
    "    for fold in range(2):\n",
    "        df_train = df_train_list[fold]\n",
    "        df_validate = df_validate_list[fold]\n",
    "\n",
    "        for num_boost_round in num_boost_round_space:\n",
    "            for max_depth in max_depth_space:\n",
    "                parameters = {\n",
    "                    \"device_type\": \"cpu\",\n",
    "                    \"nthread\": 12,\n",
    "                    \"objective\": \"reg:squarederror\",\n",
    "                    \"verbosity\": 0,\n",
    "                    \"max_depth\": max_depth,\n",
    "                    \"num_round\": num_boost_round\n",
    "                }\n",
    "\n",
    "                # use sklearn to calculate permutation feature importances (the fastest way I found so far, faster than lleaves)\n",
    "                model = xgb.XGBRegressor(\n",
    "                    **parameters\n",
    "                )\n",
    "\n",
    "                # noinspection PyTypeChecker\n",
    "                model.fit(\n",
    "                    X=df_train[feature_list_to_test],\n",
    "                    y=df_train['target']\n",
    "                )\n",
    "\n",
    "                print(\"Model trained.\")\n",
    "\n",
    "                corr = mean_grouped_spearman_correlation(\n",
    "                    pl.Series(model.predict(df_validate[feature_list_to_test])),\n",
    "                    df_validate['target'],\n",
    "                    df_validate['era']\n",
    "                )\n",
    "                corrs.append(corr)\n",
    "\n",
    "\n",
    "                def mean_correlation_by_era(target: np.ndarray, prediction: np.ndarray) -> float:\n",
    "                    return mean_grouped_spearman_correlation(pl.Series(prediction), pl.Series(target), df_validate['era'])\n",
    "\n",
    "\n",
    "                score = make_scorer(mean_correlation_by_era, greater_is_better=True)\n",
    "                warnings.filterwarnings(\"ignore\", category=UserWarning)  # supress false positive warning\n",
    "                result_permutation_importance.append(\n",
    "                    permutation_importance(\n",
    "                        model,\n",
    "                        df_validate[feature_list_to_test].to_numpy(),\n",
    "                        df_validate['target'].to_numpy(),\n",
    "                        scoring=score,\n",
    "                        n_repeats=1,\n",
    "                        n_jobs=-1  # parallelise\n",
    "                    )['importances_mean']\n",
    "                )\n",
    "\n",
    "                (DATA_PATH / 'tmp').mkdir(parents=True, exist_ok=True)\n",
    "                save_as_pickle(result_permutation_importance, DATA_PATH / 'tmp/result_permutation_importance.pkl')\n",
    "\n",
    "                print(f\"{datetime.now().strftime('%H:%M:%S')} . . . Fold {fold} with parameters num_boost_round={num_boost_round} and max_depth={max_depth} done. Current performance: {corr:.5f}.\")\n",
    "\n",
    "\n",
    "    df_feature_importance = pl.DataFrame({\n",
    "        'feature': pl.Series(feature_list_to_test),\n",
    "        'importance_permutation': pl.Series(np.array(result_permutation_importance).mean(axis=0))\n",
    "    })\n",
    "\n",
    "    max_shadow_importance = df_feature_importance.filter(pl.col('feature').str.contains('_shadow'))['importance_permutation'].max()\n",
    "\n",
    "    mean_corr = np.mean(corrs)\n",
    "    max_corr = np.max(corrs)\n",
    "    if (mean_corr + max_corr) >= (best_mean_corr + best_max_corr):\n",
    "        best_mean_corr = mean_corr\n",
    "        best_max_corr = max_corr\n",
    "        selected_features = [feature for feature in feature_list_to_test if 'shadow' not in feature]\n",
    "    else:\n",
    "        print('No performance improvement. Stopping early and using feature set of last iteration.')\n",
    "        active = False\n",
    "        break\n",
    "\n",
    "    features_to_keep_last = features_to_keep\n",
    "    features_to_keep = df_feature_importance.filter((pl.col('importance_permutation') > max_shadow_importance))['feature'].to_list()\n",
    "    features_to_keep = features_to_keep + [f'{feature}_shadow' for feature in random_features]\n",
    "\n",
    "    number_of_features_to_drop = int((len(feature_list_to_test) - len(features_to_keep)))  # excluding shadow features\n",
    "    print(f\"Dropping {number_of_features_to_drop} of {len(feature_list_to_test) - number_of_shadow_features} features\")\n",
    "\n",
    "    if number_of_features_to_drop <= int(number_of_shadow_features / 10):\n",
    "        active = False\n",
    "        print('Not enough features to drop. Stopping and using all surviving features.')\n",
    "        selected_features = [feature for feature in features_to_keep if 'shadow' not in feature]\n",
    "    else:\n",
    "        feature_list_to_test = features_to_keep"
   ],
   "id": "5358931d074bcafc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "save_as_pickle(selected_features, DATA_PATH / 'results/selected_features.pkl')",
   "id": "1875b5bc9c6f1fc8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
